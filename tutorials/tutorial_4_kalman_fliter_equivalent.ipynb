{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd0ccbd3",
   "metadata": {},
   "source": [
    "# Kalman Filter and Gaussian Processes Equivalents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad877ea7",
   "metadata": {},
   "source": [
    "This tutorial is geared towards Kalman filter experts, the goal is to draw a link between the vocabulary and model formulations used in Kalman filtering and those used in Gaussian processes. To do so we'll setup a simple 1D toy problem and demonstrate how a GP formulation can be turned into a KF and vice versa.\n",
    "\n",
    "While this tutorial takes a slightly different approach for deriving the KF equivalents to GPs, the idea was pulled heavily from these papers:\n",
    "\n",
    "[Kalman filtering and smoothing solutions to temporal Gaussian process regression models (Jouni Hartikainen Simo Särkkä)](https://ieeexplore.ieee.org/abstract/document/5589113)\n",
    "\n",
    "[Spatiotemporal Learning via Infinite Dimensional Bayesian Filtering and Smoothing [A look at Gaussian process regression through Kalman filtering] (Simo Särkkä, Arno Solin, and Jouni Hartikainen)](https://web.archive.org/web/20170829003925id_/http://www.lce.hut.fi/~asolin/documents/pdf/Sarkka-etal-2013-learning.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8522789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import scipy\n",
    "import sklearn\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tutorial_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965d73f2",
   "metadata": {},
   "source": [
    "# 1D Toy Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc17192f",
   "metadata": {},
   "source": [
    "To start let's build a random time series. To keep the example as simple as possible, let's assume that what we're trying to model is a random walk (technically an [Ornstein Uhlenbeck process](https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process), so random walk with friction). We can use an exponential covariance to build a Gaussian process for the timeseries, and can then draw a random sample from the process,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cf12c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2012)\n",
    "\n",
    "SIGMA = 10.0\n",
    "ELL = 1000.0\n",
    "MEAS_NOISE_SIGMA = 0.5\n",
    "\n",
    "x_max = 200\n",
    "X_grid = np.linspace(0., x_max, x_max+1)[:, None]\n",
    "\n",
    "def example_cov_func(x_i, x_j):\n",
    "    return example_exponential(x_i, x_j, sigma=SIGMA, ell=ELL)\n",
    "\n",
    "cov_matrix = example_cov_func(X_grid, X_grid)\n",
    "truth = np.random.multivariate_normal(np.zeros(X_grid.shape[0]), cov_matrix)\n",
    "\n",
    "def plot_truth():\n",
    "    plt.plot(X_grid, truth, color=\"black\", lw=4, label=\"truth\")\n",
    "    \n",
    "plot_truth()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972c5791",
   "metadata": {},
   "source": [
    "This will be our simulated truth. Now let's assume we've taken random measurements, $y$, of the truth at regular intervals and these measurements have been contaminated by measurement noise,\n",
    "$$\n",
    "y \\sim \\mbox{truth} + \\mathcal{N}\\left(0, \\sigma_{\\epsilon}^2\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1065c904",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_grid\n",
    "y = truth + MEAS_NOISE_SIGMA * np.random.normal(size=X.size)\n",
    "\n",
    "def plot_measurements():\n",
    "    plt.scatter(X, y, color=\"grey\", label=\"Measurement\")\n",
    "    \n",
    "plot_truth()\n",
    "plot_measurements()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698e194f",
   "metadata": {},
   "source": [
    "Now let's try to estimate the true process. We generated the truth using a GP, so we know the true covariance function and hyper parameters. We can reuse those known parameters and see what the resulting fit is. Here is a Gaussian process' posterior estimate of the true process given all the measurements,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050fd091",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_mean, batch_variance = example_fit_and_predict(example_cov_func, X, y,\n",
    "                                                     X_grid, meas_noise=MEAS_NOISE_SIGMA)\n",
    "plot_truth()\n",
    "plot_measurements()\n",
    "plot_spread(X_grid, batch_mean, np.diag(batch_variance), label=\"Batch GP\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec90ebf",
   "metadata": {},
   "source": [
    "Not bad, but we just fit a model to data which was drawn from the same model, so it isn't surprising the fit is good.\n",
    "\n",
    "Let's now assume that this is a real-time task. In other words, let's treat the x-axis as time. At each time step , $t_i$ we are presented with a new measurement, $y_i$ and would like to make a prediction of the current truth, $f_i$. The key difference here is that above, we used all observations to predict all times in one big batch process, but in a real-time scenario we would not have all the observations available, so we just use everything up to the curren time to make predictions of the truth at the curren time. Then move on to the next time step and repeat,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed887491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_time_gp(cov, X, y):\n",
    "    for i in range(X.shape[0]):\n",
    "        # at each time t_i, we can use all the observations up to the current time.\n",
    "        # to predict the i^th state.\n",
    "        available_X = X[:i+1]\n",
    "        available_y = y[:i+1]\n",
    "        m, S = example_fit_and_predict(cov, available_X, available_y,\n",
    "                                       X[i, :], meas_noise=MEAS_NOISE_SIGMA)\n",
    "        yield m[0], S[0, 0]\n",
    "\n",
    "def concatenate_mean_and_variance(estimates):\n",
    "    xs, Ps = map(np.array, zip(*estimates))\n",
    "    return xs, Ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394dfd6c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "iterative_mean, iterative_variance = concatenate_mean_and_variance(real_time_gp(example_cov_func, X, y))\n",
    "\n",
    "plot_truth()\n",
    "plot_measurements()\n",
    "ylim = plt.ylim()\n",
    "plot_spread(X, iterative_mean, iterative_variance, label=\"Iterative GP\")\n",
    "plt.ylim(ylim)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18d246e",
   "metadata": {},
   "source": [
    "This interactive plot might illustrate the process best. You can adjust the time step (i) to see what a real time model would have predicted up to that point in time (red) and what it's predictions of the future are (blue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cbb93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "\n",
    "def plot_fit_and_predict(i, cov, X, y):\n",
    "    X_train = X[:(i+1)]\n",
    "    y_train = y[:(i+1)]\n",
    "    iterative_mean, iterative_variance = concatenate_mean_and_variance(real_time_gp(cov, X_train, y_train))\n",
    "\n",
    "    X_test = X[i:, :]\n",
    "    pred_mean, pred_cov = example_fit_and_predict(cov, X_train, y_train,\n",
    "                                                  X_test, meas_noise=MEAS_NOISE_SIGMA)\n",
    "    \n",
    "    plot_spread(X_train, iterative_mean, iterative_variance, color='firebrick', label=\"Previous Prediction\")\n",
    "    plot_spread(X_test, pred_mean, np.diag(pred_cov), label=\"Forward Prediction\")\n",
    "    plot_truth()\n",
    "    plot_measurements()\n",
    "\n",
    "    ylim = [-2, 6]\n",
    "    plt.plot([i, i], ylim, ls=\":\", color=\"firebrick\")\n",
    "    plt.ylim(ylim)\n",
    "    plt.legend()\n",
    "    \n",
    "@interact\n",
    "def interactive_plot_fit_and_predict(i=(0, X.shape[0])):\n",
    "    plot_fit_and_predict(i, example_cov_func, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd31cc6",
   "metadata": {},
   "source": [
    "Compare these resulting real time predictions to the batch predictions and we can see the two are similar ... but not exactly the same,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c00220",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spread(X_grid, batch_mean, np.diag(batch_variance), color=\"firebrick\", label=\"Batch GP\")\n",
    "plot_spread(X, iterative_mean, iterative_variance, label=\"Iterative GP\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508f90f2",
   "metadata": {},
   "source": [
    "Also notice that this \"real-time\" approach would not actually be feasible in real time. As time marches forward the GP fit will be using more and more measurements and at some point the dataset will be too large, which brings us to the Kalman filter ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3630e196",
   "metadata": {},
   "source": [
    "# Kalman Filter\n",
    "For real time problems like the one described the Kalman filter is a very popular approach. Since the intended audience of this tutorial is Kalman filter expert we'll assume familiarity with the components of a KF and will focus on how to derive them. First we'll need our state, $x$, which we'll take to represent the truth at the current epoch,\n",
    "$$\n",
    "x = \\{f_i\\}\n",
    "$$\n",
    "A KF also maintains the covariance estimate of the state, $P$, such that $\\mathbf{x} \\sim \\mathcal{N}\\left(x, P\\right)$ (notice the bold font for the random variable, $\\mathbf{x}$, to differentiate it from the estimated mean, $x$). We then need a process model ($F$ and $Q$) to advance the state forward in time,\n",
    "$$\n",
    "\\mathbf{x}_{k+1|k} = F \\mathbf{x}_{k|k} + \\mathcal{N}\\left(0, Q\\right)\n",
    "$$\n",
    "In this case (brownian motion) we'll set,\n",
    "$$\n",
    "F = \\begin{bmatrix}1\\end{bmatrix} \\\\\n",
    "Q = \\begin{bmatrix}\\sigma_q^2\\end{bmatrix}\n",
    "$$\n",
    "so the next state estimate is the previous plus some process noise, $\\sigma_q^2$. We'll also need the measurement model,\n",
    "$$\n",
    "\\mathbf{y} = H \\mathbf{x} + \\mathcal{N}\\left(0, R\\right)\n",
    "$$\n",
    "which is also very simple in the example. The measurements are a direct measurement of the truth, plus some measurement noise, $\\sigma_r$,\n",
    "$$\n",
    "H = \\begin{bmatrix}1\\end{bmatrix} \\\\\n",
    "R = \\begin{bmatrix}\\sigma_r^2\\end{bmatrix}\n",
    "$$\n",
    "to initialize the algorithm we'll need a first guess at the truth, $x_0 = \\{0\\}$, and an intial variance, $P_0 = \\begin{bmatrix}\\sigma_0^2\\end{bmatrix}$.\n",
    "\n",
    "Below we'll implement a basic version of the model described. So far we've defined three hyper parameters of the KF ($\\sigma_0^2$, $\\sigma_q^2$ and $\\sigma_r^2$), we'll use these to construct the model, after which we can run the KF by iteratively applying the process model and updating with new measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421f9c79",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class MinimalKalmanFilter(object):\n",
    "    \n",
    "    def __init__(self, initial_variance=1., process_noise=0.01, meas_noise=1.0, process_model=1.0):\n",
    "        self.x = np.array([0.])\n",
    "        self.P = np.array([[initial_variance]])\n",
    "        self.Q = np.array([[process_noise]])\n",
    "        self.R = np.array([[meas_noise]])\n",
    "        self.F = np.array([[process_model]])\n",
    "        self.H = np.array([[1]])\n",
    "    \n",
    "    def process_model(self):\n",
    "        self.x = self.F @ self.x\n",
    "        self.P += self.Q\n",
    "\n",
    "    def update(self, X, y):\n",
    "        y = np.atleast_1d(y)\n",
    "        S = self.H @ self.P @ self.H.T + self.R\n",
    "        K = np.linalg.solve(S, self.H @ self.P).T\n",
    "        y_hat = self.H @ self.x\n",
    "        self.x = self.x + K @ (y - y_hat)\n",
    "        self.P = self.P  - K @ self.H @ self.P\n",
    "        \n",
    "    def predict(self):\n",
    "        return self.x, self.P\n",
    "\n",
    "        \n",
    "def run_kf(kf, X, y):\n",
    "    for i in range(X.shape[0]):\n",
    "        kf.process_model()\n",
    "        kf.update(X[i,:], y[i])        \n",
    "        x, P = kf.predict()\n",
    "        yield x[0], P[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa3fbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = MinimalKalmanFilter()\n",
    "\n",
    "kf_mean, kf_variance = concatenate_mean_and_variance(run_kf(kf, X, y))\n",
    "\n",
    "plot_truth()\n",
    "plot_measurements()\n",
    "plot_spread(X, kf_mean, kf_variance, label=\"KF\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a575a7",
   "metadata": {},
   "source": [
    "This first version of the  KF is not really performing well and certainly doesn't match the GP predictions (yet),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35309be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spread(X, kf_mean, kf_variance, label=\"KF\")\n",
    "plot_spread(X, iterative_mean, iterative_variance, color=\"firebrick\", label=\"Iterative GP\")\n",
    "plot_truth()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3891f76",
   "metadata": {},
   "source": [
    "The reason why these two are different is that we randomly picked the hyper parameters for the KF and the result is too smooth. Which leads us to the question, can we derive Kalman filter hyper parameters from a Gaussian process?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a096224",
   "metadata": {},
   "source": [
    "# Converting a Gaussian Process into a Kalman Filter\n",
    "\n",
    "In this case we have known characteristics of the truth, let's use them to derive the hyper parameters for the Kalman filter. Remember, the truth was drawn from a GP. This means we can ask some questions directly related to the process model. Namely, if you're given the truth at time $t$, what would be the truth at time $t + \\Delta$. To derive this we can use the covariance function, $k(|t - t'|)$, (which we assume is a radial covariance function, so it only depends on the time difference) to build a two variable GP prior,\n",
    "$$\n",
    "\\begin{bmatrix} f_t \\\\ f_{t+\\Delta} \\end{bmatrix}\n",
    "\\sim\n",
    "\\mathcal{N}\\left(\n",
    "\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\n",
    "\\begin{bmatrix} k(0) & k(\\Delta) \\\\ k(\\Delta) & k(0) \\end{bmatrix}\n",
    "\\right)\n",
    "$$\n",
    "Then we can use this to get the next time step given the current $f_{t + \\Delta} | f_{t}$,\n",
    "$$\n",
    "f_{t + \\Delta} | f_{t} \\sim\n",
    "\\mathcal{N} \\left(\n",
    "\\frac{k(\\Delta)}{k(0)} f_{t},\n",
    "k(0) - \\frac{k(\\Delta) k(\\Delta)}{k(0)}\n",
    "\\right)\n",
    "$$\n",
    "With an exponential covariance function, $k(\\Delta t) = \\mbox{e}^{-|\\Delta| / \\ell}$, this gives us\n",
    "$$\n",
    "f_{t + \\Delta} | f_{t} \\sim\n",
    "\\mathcal{N} \\left(\n",
    "\\mbox{e}^{-\\Delta/ \\ell}  f_{t},\n",
    "\\sigma^2 \\left(1 - \\mbox{e}^{-2\\Delta / \\ell} \\right)\n",
    "\\right)\n",
    "$$\n",
    "This let's us read off the theoretical value for $\\sigma_q$. If the state at time $t$ was perfectly know, the state at time $t+1$ would have a standard deviation of,\n",
    "$$\n",
    "\\sigma_q = \\sigma \\sqrt{1 - \\mbox{e}^{-2\\Delta / \\ell}}\n",
    "$$\n",
    "Similarly we can get the initial (prior) variance of the state which is simply,\n",
    "$$\n",
    "\\begin{align}\n",
    "P_0 &= k(0) \\\\\n",
    "&= \\sigma^2\n",
    "\\end{align}\n",
    "$$\n",
    "this lets us take the covariance function from a GP and derive the hyper parameters for a KF,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f71b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kf_equivalent_params(cov_func):\n",
    "    cov_0 = cov_func(0., 0.)[0, 0]\n",
    "    cov_dt = cov_func(0., 1.)[0, 0]\n",
    "    # YOUR CODE HERE\n",
    "    #return {\"process_noise\": process_noise,\n",
    "    #        \"initial_variance\": initial_variance}\n",
    "    \n",
    "TEST_KF_EQUILVALENT_PARAMS(kf_equivalent_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bdb4ff",
   "metadata": {},
   "source": [
    "Let's rerun the Kalman filter with these hyper parameters,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f83ba1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kf_equivalent(cov_func):\n",
    "    params = kf_equivalent_params(cov_func)\n",
    "    return MinimalKalmanFilter(meas_noise=MEAS_NOISE_SIGMA*MEAS_NOISE_SIGMA, **params)\n",
    "\n",
    "kf = kf_equivalent(example_cov_func)\n",
    "\n",
    "kf_mean, kf_variance = concatenate_mean_and_variance(run_kf(kf, X, y))\n",
    "\n",
    "plot_truth()\n",
    "plot_measurements()\n",
    "plot_spread(X, kf_mean, kf_variance, label=\"KF\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb93f61",
   "metadata": {},
   "source": [
    "Much better! Now let's compare the result with a GP,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dc74ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spread(X, kf_mean, kf_variance, label=\"KF\")\n",
    "plot_spread(X, iterative_mean, iterative_variance, color=\"firebrick\", label=\"Iterative GP\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2c6d10",
   "metadata": {},
   "source": [
    "Bingo! We built a Kalman Filter which reproduces a (real-time) GP solution. Though worth noting, we specifically picked this toy problem to make the derivation process easy, smoother Gaussian process priors will require more complicated Kalman filters (we'll get to that later)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bc7c0e",
   "metadata": {},
   "source": [
    "# Caveats\n",
    "We found a Kalman filter formulation which _very nearly_ reproduces the results that a real-time Gaussian process would but the equivalence is _not perfect_. Consider the edge case where you don't actually have any measurements. We can still make predictions which for a Gaussian process involves just using the prior,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403cf1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_mean = np.zeros(X.shape[0])\n",
    "prior_variance = np.diag(example_cov_func(X, X))\n",
    "\n",
    "plot_spread(X, prior_mean, prior_variance, color=\"firebrick\", label=\"GP prior\")\n",
    "plot_truth()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fb3c9d",
   "metadata": {},
   "source": [
    "To see how a Kalman filter behaves without measurements we can just run the process model without ever updating,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23125e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kf_prior(kf, X, y):\n",
    "    for i in range(X.shape[0]):\n",
    "        kf.process_model()\n",
    "        x, P = kf.predict()\n",
    "        yield x[0], P[0, 0]\n",
    "\n",
    "kf = kf_equivalent(example_cov_func)\n",
    "\n",
    "kf_mean, kf_variance = concatenate_mean_and_variance(kf_prior(kf, X, y))\n",
    "\n",
    "plot_spread(X, prior_mean, prior_variance, color=\"firebrick\", label=\"GP prior\")\n",
    "plot_spread(X, kf_mean, kf_variance, label=\"KF prior\")\n",
    "plot_truth()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33784918",
   "metadata": {},
   "source": [
    "See the difference? The \"prior\" for a Kalman filter increases with time. We designed the Kalman filter with a process model of $\\mbox{x}_{k+1} = \\mbox{x}_{k} + \\mathcal{N}(0, \\sigma_q^2)$. It has some initial variance, the process model simply advances the state forward in time and adds process noise. So (with these hyper parameters) when we query the Kalman filter for some time far in the future the resulting uncertainty is going to tend towards infinity! Meanwhile, in this example, we picked a Gaussian process with a stationary prior. The prior variance at time $t$ will be the same as $t + \\Delta$, even for very large $\\Delta$.\n",
    "\n",
    "There's another related phenomena which leads to a difference between these two models. Let's see what happens during a simulated measurement outage. We can start with a continuous stream of measurements, but after time 50 we'll stop updating the filter,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7ba3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTAGE_TIME = 50\n",
    "def kf_simulated_outage(kf, X, y):\n",
    "    for i in range(X.shape[0]):\n",
    "        kf.process_model()\n",
    "        if i <= OUTAGE_TIME:\n",
    "            kf.update(X[i,:], y[i])\n",
    "        x, P = kf.predict()\n",
    "        yield x[0], P[0, 0]\n",
    "        \n",
    "kf = kf_equivalent(example_cov_func)\n",
    "\n",
    "kf_outage_mean, kf_outage_variance = concatenate_mean_and_variance(kf_simulated_outage(kf, X, y))\n",
    "\n",
    "plot_spread(X, kf_outage_mean, kf_outage_variance, label=\"KF outage\")\n",
    "plot_truth()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3236babc",
   "metadata": {},
   "source": [
    "Now here's the same thing with a GP,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063019b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gp_simulated_outage(X, y):\n",
    "    for i in range(X.shape[0]):\n",
    "        # at each time t_i, we can use all the observations up to the current time.\n",
    "        # to predict the i^th state.\n",
    "        k = min(i, OUTAGE_TIME)\n",
    "        m, S = example_fit_and_predict(example_cov_func, X[:k+1], y[:k+1],\n",
    "                                       X[i, :], meas_noise=MEAS_NOISE_SIGMA)\n",
    "        yield m[0], S[0, 0]\n",
    "\n",
    "gp_outage_mean, gp_outage_variance = concatenate_mean_and_variance(gp_simulated_outage(X, y))\n",
    "\n",
    "plot_spread(X, gp_outage_mean, gp_outage_variance, color=\"firebrick\", label=\"GP outage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd77d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spread(X, gp_outage_mean, gp_outage_variance, color=\"firebrick\", label=\"GP outage\")\n",
    "plot_spread(X, kf_outage_mean, kf_outage_variance, label=\"KF outage\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e3fe47",
   "metadata": {},
   "source": [
    "See the difference? The mean estimates during the outage slowly diverge. As noted above, the prior for a stationary Gaussian process will be the same everywhere. So a GP queried a long way from any other measurements will return the prior ... which is mean zero.\n",
    "\n",
    "Note that while this particular Kalman filter will continue to return the same state during an outage, but that isn't strictly neccesary. If we're repeatedly applying the process model,\n",
    "$$\n",
    "x_{k + 1} = F x_k \\\\\n",
    "\\vdots \\\\\n",
    "x_{k + n} = F^n x_k\n",
    "$$\n",
    "So if we set $F < 1$, then the KF state will also tend back towards zero. More generally any eigen directions of $F$ for which the eigen value is $1$ will stay fixed during an outage. Any below $1$ will also converge to zero (and hopefully none are above $1$ or the filter will be unstable).\n",
    "\n",
    "So, if we truly want our Kalman filter approximation to a GP to align more closesly we can modify the process model. You may have noticed we already set the model up with a configruable process model. We can use the same derivation as before to get,\n",
    "$$\n",
    "F = \\begin{bmatrix} \\frac{k(\\Delta t)}{k(0)} \\end{bmatrix}\n",
    "$$\n",
    "Let's (re)implement the `kf_equivalent_params` method but include $F$, the process model,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c904c9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kf_equivalent_params(cov_func):\n",
    "    cov_0 = cov_func(0., 0.)[0, 0]\n",
    "    cov_dt = cov_func(0., 1.)[0, 0]\n",
    "    # YOUR CODE HERE\n",
    "    #return {\"process_noise\": process_noise,\n",
    "    #        \"initial_variance\": initial_variance,\n",
    "    #        \"process_model\": process_model}\n",
    "\n",
    "kf_equivalent_params = example_kf_equivalent_params\n",
    "\n",
    "print(kf_equivalent_params(example_cov_func))\n",
    "TEST_KF_EQUILVALENT_PARAMS(kf_equivalent_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5f0d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = kf_equivalent(example_cov_func)\n",
    "\n",
    "kf_outage_mean, kf_outage_variance = concatenate_mean_and_variance(kf_simulated_outage(kf, X, y))\n",
    "\n",
    "plot_spread(X, gp_outage_mean, gp_outage_variance, color=\"firebrick\", label=\"GP outage\")\n",
    "plot_spread(X, kf_outage_mean, kf_outage_variance, label=\"KF outage\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acce501",
   "metadata": {},
   "source": [
    "Now the mean predictions during an outage match (but the KF's variance still tends towards infinity)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b5954f",
   "metadata": {},
   "source": [
    "# Gaussian Process from Kalman Filter?\n",
    "We just showed that (for a very specific) covariance function we can find a Kalman Filter which approximately reproduces real-time GP performance. Can we do the opposite? Lets consider a generic Kalman Filter with a process model,\n",
    "$$\n",
    "\\mathbf{x}_{k+1|k} = F \\mathbf{x}_{k|k} + \\mathcal{N}\\left(0, Q\\right)\n",
    "$$\n",
    "and measurement model,\n",
    "$$\n",
    "\\mathbf{y}_{k} = H \\mathbf{x}_{k} + \\mathcal{N}\\left(0, R\\right)\n",
    "$$\n",
    "We can use the process model to compute the prior between two times,\n",
    "$$\n",
    "\\begin{align}\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{x}_{k}\\\\\n",
    "\\mathbf{x}_{k+1}\n",
    "\\end{bmatrix} &= \n",
    "  \\begin{bmatrix} I \\\\ F \\end{bmatrix} \\mathcal{N}\\left(0, P_0\\right) + \\mathcal{N}\\left(0, \\begin{bmatrix}\n",
    "     0 & 0 \\\\\n",
    "     0 & Q\n",
    "   \\end{bmatrix}\\right) \\\\\n",
    "&\\sim\n",
    "\\mathcal{N}\\left(\n",
    "0, \\begin{bmatrix}\n",
    "     P_0 & P_0 F^T \\\\\n",
    "     F P_0 & F P_0 F^T + Q\n",
    "   \\end{bmatrix}\n",
    "\\right)\n",
    "\\end{align}\n",
    "$$\n",
    "For now let's stick with the 1D toy problem we have here, so we can assume all these variables are scalars, specifically $P_0 = \\sigma^2$, $F = \\alpha$ and $Q = \\gamma^2$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34595952",
   "metadata": {},
   "source": [
    "Using those simplifications and repeating again,\n",
    "$$\n",
    "\\begin{align}\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{x}_{k}\\\\\n",
    "\\mathbf{x}_{k+1}\\\\\n",
    "\\mathbf{x}_{k+2}\n",
    "\\end{bmatrix} &=\n",
    "\\begin{bmatrix} 1 & 0\\\\ 0 & 1 \\\\ 0 & \\alpha \\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{x}_{k}\\\\\n",
    "\\mathbf{x}_{k+1}\n",
    "\\end{bmatrix} + \\mathcal{N}\\left(0, \\begin{bmatrix}\n",
    "     0 & 0 & 0\\\\\n",
    "     0 & 0 & 0\\\\\n",
    "     0 & 0 & Q\n",
    "   \\end{bmatrix}\\right) \\\\\n",
    "&\\sim\n",
    "\\mathcal{N}\\left(\n",
    "0, \\begin{bmatrix}\n",
    "     \\sigma^2 & \\alpha \\sigma^2 & \\alpha^2 \\sigma^2 \\\\\n",
    "     \\alpha \\sigma^2 & \\alpha^2 \\sigma^2 + \\gamma^2 & \\alpha (\\alpha^2 \\sigma^2 + \\gamma^2) \\\\\n",
    "     \\alpha^2 \\sigma^2 & \\alpha (\\alpha^2 \\sigma^2 + \\gamma^2)  & \\alpha^4 \\sigma^2 + \\alpha^2 \\gamma^2 + \\gamma^2\n",
    "   \\end{bmatrix}\n",
    "\\right)\n",
    "\\end{align}\n",
    "$$\n",
    "and we begin to see a pattern. Along the diagonal we have the marginal prior,\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mbox{cov}(t+\\Delta, t+\\Delta) &= \\alpha^{2\\Delta } \\sigma^2 + \\gamma^2 \\sum_{i=0}^{\\Delta-1} \\alpha^i \\\\\n",
    "&= \\alpha^{2\\Delta} \\sigma^2 + \\left(\\frac{1 - \\alpha^{2\\Delta}}{1 - \\alpha^2}\\right) \\gamma^2\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80aedea2",
   "metadata": {},
   "source": [
    "along the off diagonals we'll have,\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mbox{cov}(t+\\Delta, t+\\Delta+\\delta) &= \\alpha^\\delta \\mbox{cov}(t+\\Delta, t+\\Delta)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4859bb07",
   "metadata": {},
   "source": [
    "The covariance function is NOT stationary, because we cannot express the covariance function simply as a functio of the temporal distance, $\\mbox{cov}(t, t^\\prime) = k(|t - t^\\prime|)$. Instead, the covariance is a function of the time. Let's give it a try. Here we'll write a new covariance function which _should_ give us a GP which provides the same reults as a KF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaec7f37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_equivalent_cov_func(sigma, alpha, gamma):\n",
    "\n",
    "    def cov_func(x_i, x_j):\n",
    "        # resize the inputs\n",
    "        x_i = np.expand_dims(reshape_inputs(x_i), 1)\n",
    "        x_j = np.expand_dims(reshape_inputs(x_j), 0)\n",
    "        output = np.zeros((x_i.shape[0], x_j.shape[1]))\n",
    "        for i in range(output.shape[0]):\n",
    "            for j in range(output.shape[1]):\n",
    "                i_val = x_i[i, :].item()\n",
    "                j_val = x_j[:, j, :].item()\n",
    "                # offset = \\Delta above\n",
    "                offset = min(i_val, j_val)\n",
    "                b = max(i_val, j_val)\n",
    "                # d = \\delta\n",
    "                d = b - offset\n",
    "                a2offset = np.power(alpha, 2*offset)\n",
    "                frac = (1 - a2offset) / (1 - alpha * alpha)\n",
    "                ad = np.power(alpha, d)\n",
    "                marginal = a2offset * np.square(sigma) + frac * np.square(gamma)\n",
    "                output[i, j] = np.power(alpha, d) * marginal\n",
    "        return output\n",
    "    return cov_func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee197b9",
   "metadata": {},
   "source": [
    "Just to be sure we aren't cheating, we'll change the hyper parameters for the KF a little bit,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1078bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_variance = 100.\n",
    "process_noise = 0.5\n",
    "process_model = 0.9999\n",
    "\n",
    "kf = MinimalKalmanFilter(initial_variance=initial_variance,\n",
    "                               process_noise=process_noise,\n",
    "                               meas_noise=MEAS_NOISE_SIGMA*MEAS_NOISE_SIGMA,\n",
    "                               process_model=process_model)\n",
    "\n",
    "kf_mean, kf_variance = concatenate_mean_and_variance(run_kf(kf, X, y))\n",
    "\n",
    "cov_func = get_equivalent_cov_func(sigma=np.sqrt(initial_variance),\n",
    "                                   alpha=process_model,\n",
    "                                   gamma=np.sqrt(process_noise))\n",
    "gp_equivalent_mean, gp_equivalent_variance = concatenate_mean_and_variance(real_time_gp(cov_func, X, y))\n",
    "\n",
    "plot_spread(X, kf_mean, kf_variance, color=\"steelblue\", label=\"KF\")\n",
    "plot_spread(X, gp_equivalent_mean, gp_equivalent_variance, color=\"firebrick\", label=\"GP Equivalent\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789329f8",
   "metadata": {},
   "source": [
    "Not the most efficent implementation, but the results match. If we want to implement a Kalman smoother (not a filter), we can just re-use that covariance function with a batch GP,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db60eaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "smoother_mean, smoother_covariance = example_fit_and_predict(cov_func, X, y,\n",
    "                                                                     X_grid, meas_noise=MEAS_NOISE_SIGMA)\n",
    "plot_spread(X, kf_mean, kf_variance, color=\"steelblue\", label=\"KF\")\n",
    "plot_spread(X, smoother_mean, np.diag(smoother_covariance), color=\"firebrick\", label=\"KF Smoother Equivalent\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220b0bba",
   "metadata": {},
   "source": [
    "# Equivalence with Smooth Processes\n",
    "In the previous example we used a random walk which is known to satisfy the markov assumption required by a Kalman filter, namely: the next state is only a function of the previous state. This made the conversion from GP to KF easy, so what would we need to do differently if the process we're modelling is smooth?\n",
    "\n",
    "We can setup a very similar experiment, but this time we'll draw the truth from a squared exponential covariance function,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24eb7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2012)\n",
    "\n",
    "def smooth_cov(x_i, x_j):\n",
    "    return example_squared_exponential(x_i, x_j, sigma=5., ell=1000)\n",
    "\n",
    "cov_matrix = smooth_cov(X_grid, X_grid)\n",
    "truth = np.random.multivariate_normal(np.zeros(X_grid.shape[0]), cov_matrix)\n",
    "\n",
    "y = truth + MEAS_NOISE_SIGMA * np.random.normal(size=X.size)\n",
    "\n",
    "plot_truth()\n",
    "plot_measurements()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8116ce8a",
   "metadata": {},
   "source": [
    "Again we can run a real time GP on the simulated data,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005e036d",
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_mean, smooth_variance = concatenate_mean_and_variance(real_time_gp(smooth_cov, X, y))\n",
    "\n",
    "plot_truth()\n",
    "plot_measurements()\n",
    "\n",
    "plot_spread(X, smooth_mean, smooth_variance, color=\"firebrick\", label=\"GP\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036192d5",
   "metadata": {},
   "source": [
    "And can use the Kalman filter equivalence formula from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae670dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = kf_equivalent(smooth_cov)\n",
    "\n",
    "kf_mean, kf_variance = concatenate_mean_and_variance(run_kf(kf, X, y))\n",
    "\n",
    "plot_truth()\n",
    "plot_measurements()\n",
    "plot_spread(X, kf_mean, kf_variance, label=\"KF\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9c5f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spread(X, smooth_mean, smooth_variance, color=\"firebrick\", label=\"GP\")\n",
    "plot_spread(X, kf_mean, kf_variance, label=\"KF\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6411a3",
   "metadata": {},
   "source": [
    "But wait! The two are no longer the same! The difference stems from the fact that the GP has access to the full history of measurements which all collaborate to make a prediction of the current state, while the KF keeps track of only the most recent estimate of the state, so it can't enforce the same smoothness assumptions. We can improve this by adding a velocity term to the KF so we have,\n",
    "$$\n",
    "x_t = \\begin{bmatrix} f(t) \\\\ \\frac{\\partial f}{\\partial t}(t) \\end{bmatrix}\n",
    "$$\n",
    "with a process model that looks something like\n",
    "$$\n",
    "F = \\begin{bmatrix}\n",
    "1 & \\Delta t \\\\\n",
    "0 & 1\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2302ba",
   "metadata": {},
   "source": [
    "We can still use the covariance function to derive the Kalman filter parameters, though it gets a little more complex. We can use a finite element inspired approach to first describe the state in the neighborhood of $x_t$. We'll use central differencing, so we can evaluate the covariance function at three points,\n",
    "$$\n",
    "\\xi_t = [x_t - \\epsilon, x_t, x_t + \\epsilon] \\\\\n",
    "\\Sigma = k(\\xi_t, \\xi_t)\n",
    "$$\n",
    "This gives us,\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "f(x_t - \\epsilon) \\\\ f(x_t) \\\\ f(x_t + \\epsilon)\n",
    "\\end{bmatrix}\n",
    "\\sim\n",
    "\\mathcal{N}\\left(\n",
    "0, \\Sigma\n",
    "\\right)\n",
    "$$\n",
    "and then we can take the central differencing operator which says that,\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial f}{\\partial t}(t) \\approx \\frac{ f(x_t + \\epsilon) - f(x_t - \\epsilon)}{2 \\epsilon} \\\\\n",
    "\\frac{\\partial f}{\\partial t}(t) \\approx \\begin{bmatrix} -0.5 & 0 & 0.5 \\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "f(x_t - \\epsilon) \\\\ f(x_t) \\\\ f(x_t + \\epsilon)\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "Similarly we can extract only the central term with a vector, $\\begin{bmatrix} 0 & 1 & 0 \\end{bmatrix}$, so we can write our Kalman filter state as a function of the finite element approximation,\n",
    "$$\n",
    "\\begin{align}\n",
    "\\begin{bmatrix} f(t) \\\\ \\frac{\\partial f}{\\partial t}(t) \\end{bmatrix} &\\approx\n",
    "\\begin{bmatrix}\n",
    "0 & 1 & 0 \\\\\n",
    "-0.5 & 0 & 0.5\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "f(x_t - \\epsilon) \\\\ f(x_t) \\\\ f(x_t + \\epsilon)\n",
    "\\end{bmatrix} \\\\\n",
    "&\\approx C \\begin{bmatrix}\n",
    "f(x_t - \\epsilon) \\\\ f(x_t) \\\\ f(x_t + \\epsilon)\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "Which then lets us evaluate the initial state,\n",
    "$$\n",
    "\\mathbf{x} \\sim \\mathcal{N}\\left(\n",
    "0, C \\Sigma C^T\n",
    "\\right)\n",
    "$$\n",
    "Then, like before, we can stack the current state and the next state to help derive the process model,\n",
    "$$\n",
    "\\begin{align}\n",
    "\\begin{bmatrix}\n",
    "  x_t \\\\ x_{t+\\Delta t}\n",
    "\\end{bmatrix} &=\n",
    "\\begin{bmatrix}\n",
    "C & 0 \\\\\n",
    "0 & C\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "f(x_t - \\epsilon) \\\\ f(x_t) \\\\ f(x_t + \\epsilon) \\\\\n",
    "f(x_t + \\Delta t - \\epsilon) \\\\ f(x_t + \\Delta t) \\\\ f(x_t + \\Delta t + \\epsilon)\n",
    "\\end{bmatrix}\\\\\n",
    "&\\sim\n",
    "\\begin{bmatrix}\n",
    "C & 0 \\\\\n",
    "0 & C\n",
    "\\end{bmatrix} \\mathcal{N}\n",
    "  \\left(0,\n",
    "    \\begin{bmatrix} \\Sigma & \\Sigma_{\\Delta t} \\\\\n",
    "                    \\Sigma_{\\Delta t} & \\Sigma\n",
    "    \\end{bmatrix}\n",
    "  \\right)\\\\\n",
    "& \\sim\n",
    "\\mathcal{N}\n",
    "  \\left(0,\n",
    "    \\begin{bmatrix} C \\Sigma C^T & C \\Sigma_{\\Delta t} C^T \\\\\n",
    "                    C \\Sigma_{\\Delta t} C^T & C \\Sigma C^T\n",
    "    \\end{bmatrix}\n",
    "  \\right) \\\\\n",
    "& \\sim\n",
    "\\mathcal{N}\n",
    "  \\left(0,\n",
    "    \\begin{bmatrix} K_0 & K_{\\Delta} \\\\\n",
    "                    K_{\\Delta}^T & K_0\n",
    "    \\end{bmatrix}\n",
    "  \\right)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa532e6",
   "metadata": {},
   "source": [
    "And again, we can use this get $x_{t+\\Delta t}$ given $x_t$,\n",
    "$$\n",
    "x_{t+\\Delta t} \\sim \\mathcal{N}\\left(\n",
    "K_\\Delta K_0^{-1} x_t, K_0 - K_\\Delta K_0^{-1} K_\\Delta^T\n",
    "\\right)\n",
    "$$\n",
    "Which gives us,\n",
    "$$\n",
    "F = K_\\Delta K_0^{-1}\n",
    "$$\n",
    "and \n",
    "$$\n",
    "Q = K_0 - K_\\Delta K_0^{-1} K_\\Delta^T\n",
    "$$\n",
    "Basically the same idea as before, but now we're dealing with matrices, not scalars. Let's actually compute those for this example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2858c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_func = smooth_cov\n",
    "\n",
    "def derive_kalman_filter_matrices(cov_func, order):\n",
    "    epsilon = 1e-3\n",
    "\n",
    "    offsets = np.linspace(-order, order, 2*order + 1)\n",
    "    x_perturbed = epsilon * offsets\n",
    "    x_shifted = x_perturbed + 1\n",
    "\n",
    "    C = np.array([\n",
    "        np.power(epsilon, -i) * scipy.misc.central_diff_weights(x_perturbed.size, i)\n",
    "        for i in range(order + 1)\n",
    "    ])\n",
    "\n",
    "    S = cov_func(x_perturbed, x_perturbed)\n",
    "    S_delta = cov_func(x_perturbed, x_shifted).T\n",
    "\n",
    "    K_0 = C @ S @ C.T\n",
    "    K_delta = C @ S_delta @ C.T\n",
    "\n",
    "    P_init = K_0\n",
    "    F = np.linalg.solve(K_0, K_delta.T).T\n",
    "    Q = K_0 - K_delta @ np.linalg.solve(K_0, K_delta.T)\n",
    "    Q = Q + 1e-8 * np.eye(Q.shape[0])\n",
    "\n",
    "    return P_init, F, Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3c077d",
   "metadata": {},
   "source": [
    "Then we need a new KF implementation which keeps track of two states, the truth and it's derivative,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e69484",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmoothKalmanFilter(object):\n",
    "    \n",
    "    def __init__(self, order, P_init, F, Q, meas_noise=1.0):\n",
    "        self.x = np.zeros((order + 1, 1))\n",
    "        self.P = P_init\n",
    "        self.F = F\n",
    "        self.Q = Q\n",
    "        self.H = np.zeros((1, order + 1))\n",
    "        self.H[0, 0] = 1\n",
    "        self.R = np.array([[meas_noise]])\n",
    "    \n",
    "    def process_model(self):\n",
    "        self.x = self.F @ self.x\n",
    "        self.P = self.F @ self.P @ self.F.T + self.Q\n",
    "\n",
    "    def update(self, X, y):\n",
    "        y = np.atleast_1d(y)\n",
    "        S = self.H @ self.P @ self.H.T + self.R\n",
    "        K = np.linalg.solve(S, self.H @ self.P).T\n",
    "        y_hat = self.H @ self.x\n",
    "        self.x = self.x + K @ (y - y_hat)\n",
    "        self.P = self.P  - K @ self.H @ self.P\n",
    "        \n",
    "    def predict(self):\n",
    "        return self.x[:1], self.P[[0, 0]]\n",
    "    \n",
    "def derive_kalman_filter(smooth_cov, order):\n",
    "    P_init, F, Q = derive_kalman_filter_matrices(smooth_cov, order=order)\n",
    "    return SmoothKalmanFilter(order, P_init, F, Q, meas_noise=MEAS_NOISE_SIGMA*MEAS_NOISE_SIGMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fca0d2",
   "metadata": {},
   "source": [
    "And then we can compare the two, which _should_ mostly agree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc506143",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = derive_kalman_filter(smooth_cov, order=1)\n",
    "kf_mean, kf_variance = concatenate_mean_and_variance(run_kf(kf, X, y))\n",
    "\n",
    "batch_mean, batch_variance = example_fit_and_predict(smooth_cov, X, y,\n",
    "                                                     X, meas_noise=MEAS_NOISE_SIGMA)\n",
    "\n",
    "plot_spread(X, smooth_mean, smooth_variance, color=\"firebrick\", label=\"GP\")\n",
    "plot_spread(X, kf_mean, kf_variance, label=\"KF\")\n",
    "plot_truth()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90331d5c",
   "metadata": {},
   "source": [
    "In this case it only seems to have taken one derivative in the kalman filter to replicate the squared exponential covariance. In theory it would require infinite derivatives to perfectly match ... but as pointed out in [Kalman filtering and smoothing solutions to temporal Gaussian process regression models (Jouni Hartikainen Simo Särkkä)](https://ieeexplore.ieee.org/abstract/document/5589113)\n",
    "```\n",
    "With N = 2 the tails of the density deviate from the true values, while with N = 4 there is only some difference and with N = 6 the approximate density cannot be easily distinguished from the exact value.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a98be56",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "We've shown that for a few toy problems we can convert a GP into a KF and vice versa. This just touches the surface on equivalencies between various covariance functions, Kalman filter equivalents and ways to frame them both as Stochastic PDEs. For more I'd strongly suggest this thesis:\n",
    "\n",
    "[Stochastic Differential Equation Methods for Spatio-Temporal Gaussian Process Regression - Arno Solin\n",
    "](https://aaltodoc.aalto.fi/server/api/core/bitstreams/aaf7725c-7955-4d21-8d31-e27fdd23c503/content)\n",
    "\n",
    "As mentioned, the \"real time\" GP is not actually feasible in real time, it would quickly grow too large to manage. However, using the sparse Gaussian process approch from previous tutorials we can end up with a relatively efficient approximation. Perhaps a topic for a future tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3a2262",
   "metadata": {},
   "source": [
    "# Appendix: Random Insight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce69667b",
   "metadata": {},
   "source": [
    "One interesting thing that came up when writing this tutorial was despite having picked an extremely smooth covariance function the real time predictions using the GP are not particularly smooth. To see why this happens try playing around with this interactive plot which shows the \"real time\" GP predictions up to a certain time and the forward looking predictions using the GP fit up to that time. You can see the forward looking predictions are always very smooth, but the immediate prediction is influenced by recent observations which cause the predictions to wander around a little more than you may expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5627f8ea",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "@interact\n",
    "def interactive_plot_fit_and_predict(i=(0, X.shape[0])):\n",
    "    plot_fit_and_predict(i, smooth_cov, X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3491917",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
