{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6298f495-1689-4270-8cae-73a4314a6e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "import scipy\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ipywidgets import interact\n",
    "from functools import partial\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from tutorial_utils import *\n",
    "\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3e0d7e",
   "metadata": {},
   "source": [
    "In the first tutorial we established `fit` and `predict` methods for dense/direct/traditional Gaussian processes regression. These methods are relatively fast for small datasets, but scale cubicly in the number of data points, $\\mathcal{O}(n^3)$. To see this we've added a helper function which executes some of our fit / predict methods for different sized problems.\n",
    "\n",
    "We can use this helper to time how long it takes to simply evaluate the covariance matrix between training points and all others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e3da8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure the covariance function evaluation\n",
    "def build_cov(cov_func, X, y, x_test, meas_noise):\n",
    "    return cov_func(X, X)\n",
    "sizes, cov_times = zip(*generate_timings(build_cov))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b48553",
   "metadata": {},
   "source": [
    "Then we can measure how long the fit method takes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7090177e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure the fit step.\n",
    "def do_fit(cov_func, X, y, x_test, meas_noise):\n",
    "    return example_fit(cov_func, X, y, meas_noise=meas_noise)\n",
    "sizes, fit_times = zip(*generate_timings(do_fit))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d122b0e9",
   "metadata": {},
   "source": [
    "And again, but for the predict step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdea55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure the predict step.\n",
    "dummy_L = np.tril(np.random.normal(size=(10000, 10000)))\n",
    "def do_predict(cov_func, X, y, x_test, meas_noise):\n",
    "    dummy_fit = {\n",
    "        \"train_locations\": X,\n",
    "        \"information\": y,\n",
    "        \"cholesky\": dummy_L[:y.size,:y.size],\n",
    "        \"cov_func\": cov_func\n",
    "    }\n",
    "\n",
    "    return example_predict(dummy_fit, x_test)\n",
    "\n",
    "sizes, predict_times = zip(*generate_timings(do_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af365746",
   "metadata": {},
   "source": [
    "Notice that as the number of measurements increases the fit step becomes increasingly complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6468f3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the fit step requires building the covariance and performing the solve.\n",
    "solve_times = np.array(fit_times) - cov_times\n",
    "\n",
    "plt.plot(sizes, solve_times, label=\"solve\")\n",
    "plt.plot(sizes, cov_times, label=\"cov eval\")\n",
    "plt.plot(sizes, predict_times, label=\"predict\")\n",
    "plt.xlabel(\"Number of Measurements\")\n",
    "plt.ylabel(\"Seconds\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b42b20",
   "metadata": {},
   "source": [
    "The exact timing you'll see obviously depend on your machine, but you should see that the trend is not linear. Extrapolate those fit times out to something like 100,000 measurements and it's going to take a _VERY_ long time to fit a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0b6188",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_sizes = [10000, 50000, 100000]\n",
    "extrapolated_solve_times = np.polyval(np.polyfit(sizes, solve_times, 3), query_sizes)\n",
    "pd.Series(extrapolated_solve_times / 60., query_sizes, name=\"Expected Solve Time (minutes)\").to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba15be2",
   "metadata": {},
   "source": [
    "Yikes, my machine says one hour to train with 100k measurements. Thankfully there are a lot of techniques which focus on mitigating the $\\mathcal{O}(n^3)$ bottleneck. For a great review see [When Gaussian Process Meets Big Data: A Review of Scalable GPs](https://arxiv.org/abs/1807.01065). Here we describe the Fully Independent Training Conditional approach (FITC).\n",
    "\n",
    "These Sparse methods (FITC and its relatives) rely heavily on the Nystrom approximation which can be used to decribe some large dense symmetric matrix $A \\in \\mathbb{R}^{n, n}$ as a composition of low rank matrices,\n",
    "$$\n",
    "\\begin{align}\n",
    "A &= B C^{-1} B^T \\\\\n",
    "&= \\begin{bmatrix}\n",
    "B_0 \\\\\n",
    "\\vdots \\\\\n",
    "B_N\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "C^{-1}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "B_0 &\n",
    "\\cdots &\n",
    "B_N\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "where $B \\in \\mathbb{R}^{n, m}$ is a tall skinny matrix of rank $m$ and $C \\in \\mathbb{R}^{m, m}$ is a smaller symmetric matrix. To help build some intuition around the equation, let's consider the case where we think our data is described by a very simple model.\n",
    "\n",
    "Remember that we wrote the measurements, $\\mathbf{y}$, in terms of the unknown function, $f(x)$ and noise, $\\mathbf{\\epsilon} \\sim \\mathcal{N}\\left(0, \\sigma_{\\epsilon}^2\\right)$,\n",
    "$$\n",
    "\\mathbf{y} = f(x) + \\mathbf{\\epsilon}\n",
    "$$\n",
    "Let's say that we think $f(x) = a$. In otherwords, we think the unknown function is a constant which doesn't even depend on $x$. We don't know the value of $a$ though and would like to estimate it. We can place a prior on its magnitude: $\\mathbf{a} \\sim \\mathcal{N}(0, \\sigma_a^2)$. The covariance function for a constant function is relatively simple, no matter which arguments you provide the covariance is always the same,\n",
    "$$\n",
    "c(x, x') = \\sigma_a^2\n",
    "$$\n",
    "We can then use the covariance fuction to build the prior,\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{y} & \\sim \\mathcal{N}\\left(0, \\Sigma_{ff} + \\sigma_{\\epsilon}^2 I\\right) \\\\\n",
    "& \\sim \\mathcal{N}\\left(0,\n",
    "\\begin{bmatrix}\n",
    "  \\sigma_a^2 & \\ldots & \\sigma_a^2 \\\\\n",
    "  \\vdots & \\ddots & \\vdots \\\\\n",
    "  \\sigma_a^2 & \\ldots & \\sigma_a^2\n",
    "\\end{bmatrix} + \\sigma_{epsilon}^2 I\\right)\n",
    "\\end{align}\n",
    "$$\n",
    "In this case $\\Sigma_{ff}$ has the same value everywhere, it's $n$ by $n$ but is only rank 1, so it isn't invertible.  We _could_ directly plug $\\Sigma_{ff} + \\sigma_{\\epsilon}^2 I$ into the equation for the posterior,\n",
    "$$\n",
    "\\mbox{E}\\left[\\mathbf{f}^*|y\\right] =\\Sigma_{*f} \\left(\\mathbb{1} \\sigma_a^{2} \\mathbb{1}^T + \\sigma_{\\epsilon}^2 I\\right)^{-1} y\n",
    "$$\n",
    "Here's an example of what such a prediction would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6421df",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X, y = generate_training_data(1000)\n",
    "SIGMA_CONSTANT = 100.\n",
    "MEAS_NOISE_CONSTANT = 20.\n",
    "\n",
    "def constant_covariance(x_i, x_j, sigma_constant=SIGMA_CONSTANT):\n",
    "    m = np.atleast_1d(x_i).shape[0]\n",
    "    n = np.atleast_1d(x_j).shape[0]\n",
    "    return sigma_constant * sigma_constant * np.ones((m, n))\n",
    "\n",
    "x_gridded = np.linspace(LOWEST, HIGHEST, 101)\n",
    "mean_pred, cov_pred = example_fit_and_predict(constant_covariance, X, y,\n",
    "                                              x_gridded, meas_noise=MEAS_NOISE_CONSTANT)\n",
    "\n",
    "plot_measurements(X, y)\n",
    "plot_truth(x_gridded)\n",
    "plot_spread(x_gridded, mean_pred, np.sqrt(np.diag(cov_pred)))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6806b7e1",
   "metadata": {},
   "source": [
    "Clearly a constant is not a good fit (but that's not the point), let's see how long it took to fit and predict,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a11e8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_star = np.array([0.])\n",
    "%timeit example_fit_and_predict(constant_covariance, X, y, x_star, meas_noise=MEAS_NOISE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbac3965",
   "metadata": {},
   "source": [
    "Compare that with the squared exponential,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1267f7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit example_fit_and_predict(example_squared_exponential, X, y, x_star, meas_noise=MEAS_NOISE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9bb6cc",
   "metadata": {},
   "source": [
    "You should see slight timing differences, probably the constant function is faster (because the covariance function is faster to evaluate) but you should see the same order of magnitude because in both approaches the prior covariance matrix that we need to invert, $\\left(\\mathbb{1} \\sigma_a^{2} \\mathbb{1}^T + \\sigma_{\\epsilon}^2 I\\right)$, is an $n$ by $n$ matrix. Considering how much simpler the constant function is we'd hope it would be very easy to estimate.\n",
    "\n",
    "Thankfully, we can use the [matrix inversion lemma](https://en.wikipedia.org/wiki/Woodbury_matrix_identity) which states that for invertible matrices $A$ and $C$,\n",
    "$$\n",
    "\\left(A + U C U^T\\right)^{-1} = A^{-1} - A^{-1} U \\left(C^{-1} + U^T A^{-1} U\\right)^{-1} U^T A^{-1}\n",
    "$$\n",
    "At first this may not appear to help, but if $A$ is easy to invert (diagonal matrix for example) then using the lemma can lead to much faster solves. In this case, by setting $A = \\sigma_{\\epsilon}^2 I$ with $U = \\mathbb{1}$ and $C = \\sigma_a^{2}$, we can arrive at an extremely efficient solver,\n",
    "$$\n",
    "\\begin{align}\n",
    "\\left(\\sigma_{\\epsilon}^2 I + \\mathbb{1} \\sigma_a^{2} \\mathbb{1}^T\\right)^{-1} &=\n",
    "\\sigma_{\\epsilon}^{-2} I - \\sigma_{\\epsilon}^{-2} I \\mathbb{1} \\left(\\sigma_a^{-2} + \\mathbb{1}^T \\sigma_{\\epsilon}^{-2} I \\mathbb{1}\\right)^{-1} \\mathbb{1}^T \\sigma_{\\epsilon}^{-2} I \\\\\n",
    "&= \\sigma_{\\epsilon}^{-2} I - \\sigma_{\\epsilon}^{-2} \\mathbb{1} \\left(\\sigma_a^{-2} + n \\sigma_{\\epsilon}^{-2}\\right)^{-1} \\mathbb{1}^T \\sigma_{\\epsilon}^{-2} \\\\\n",
    "&= \\sigma_{\\epsilon}^{-2} \\left(I - \\frac{\\sigma_{\\epsilon}^{-2}}{\\sigma_a^{-2} + n \\sigma_{\\epsilon}^{-2}} \\mathbb{1} \\mathbb{1}^T \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "Notice that in this example $C$ is a one by one matrix, so we could do all the matrix inversions analytically. Let's actually implement a fit/predict for this for the special case. We can break the steps down into, first compute $\\gamma$,\n",
    "$$\n",
    "\\gamma = \\frac{\\sigma_{\\epsilon}^{-2}}{\\sigma_a^{-2} + n \\sigma_{\\epsilon}^{-2}} \n",
    "$$\n",
    "Then use that to get\n",
    "$$\n",
    "\\left(\\Sigma_{ff} + \\sigma_{\\epsilon}^2 I\\right)^{-1} y = \\sigma_{\\epsilon}^{-2} \\left(I - \\gamma \\mathbb{1} \\mathbb{1}^T \\right)y\n",
    "$$\n",
    "Notice that $\\mathbb{1}^T y = \\sum y_i = \\mbox{sum}[y]$, so,\n",
    "$$\n",
    "\\sigma_{\\epsilon}^{-2} \\left(I - \\gamma \\mathbb{1} \\mathbb{1}^T \\right)y = \\sigma_{\\epsilon}^{-2}\\left( y - \\gamma \\mbox{sum}[y]\\right)\n",
    "$$\n",
    "and finally we multiply through by $\\Sigma_{*f} = \\sigma_a^2 \\mathbb{1}^T$ which involves another sum,\n",
    "$$\n",
    "\\frac{\\sigma_a^2}{\\sigma_{\\epsilon}^{2}} \\mbox{sum}\\left[y - \\gamma \\mbox{sum}[y]\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58db2f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def direct_fit_predict_constant(X, y, sigma_constant, meas_noise):\n",
    "    # YOUR CODE HERE\n",
    "    # gamma = sigma_e^{-2} / (sigma_a^{-2} + n sigma_e^{-2})\n",
    "    # gamma = \n",
    "    # ratio = sigma_a^2 / sigma_e^2\n",
    "    # ratio = \n",
    "    # mean_prediction = \n",
    "    return mean_prediction # (scalar)\n",
    "\n",
    "# example call for debugging\n",
    "#X_small, y_small = generate_training_data(3)\n",
    "#example_direct_fit_predict_constant(X_small, y_small, SIGMA_CONSTANT, MEAS_NOISE)\n",
    "\n",
    "TEST_DIRECT_FIT_PREDICT(direct_fit_predict_constant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b924a362",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit direct_fit_predict_constant(X, y, SIGMA_CONSTANT, MEAS_NOISE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669dd395",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_mean, _ = example_fit_and_predict(constant_covariance, X, y, x_star, meas_noise=MEAS_NOISE)\n",
    "direct_mean = direct_fit_predict_constant(X, y, SIGMA_CONSTANT, MEAS_NOISE)\n",
    "print(f\"Dense Mean:  {dense_mean[0]}\\nSparse Mean: {direct_mean}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cdda38",
   "metadata": {},
   "source": [
    "Same result, but something like 1000x faster!\n",
    "\n",
    "But of course, the assumption that the function was constant was awful. To generalize this and use vocabulary more common in litterature, we'd call $u = [a]$ our inducing points. The inducing points are the basis for a low rank approximation, the name is descriptive, inducing points are like filter state which transfer (or induce) information from measurements to predictions. It's common to use $Q$ to represent the covariance captured by the inducing points,\n",
    "$$\n",
    "\\begin{align}\n",
    "Q_{ij} &\\approx \\Sigma_{iu} \\Sigma_{uu}^{-1} \\Sigma_{uj}\n",
    "\\end{align}\n",
    "$$\n",
    "And we can then write our approximate prior on $\\mathbf{y}$ as,\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Sigma_{yy} &\\approx \\Sigma_{fu} \\Sigma_{uu}^{-1} \\Sigma_{uf} + D \\\\\n",
    "& = Q_{ff} + D\n",
    "\\end{align}\n",
    "$$\n",
    "Here we've added $D$ which is a diagonal matrix capturing any of the covariance that was not captured using the inducing points,\n",
    "$$\n",
    "\\begin{align}\n",
    "D &= \\mbox{diag}\\left(\\Sigma_{yy} - Q_{ff}\\right) \\\\\n",
    "  &= \\begin{bmatrix}\n",
    "  \\Sigma_{00} - Q_{00} + \\sigma_{\\epsilon}^2 & 0 & 0 \\\\\n",
    "  0 & \\ddots & 0 \\\\\n",
    "  0 & 0 & \\Sigma_{nn} - Q_{nn} + \\sigma_{\\epsilon}^2 \\\\\n",
    "  \\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "If we plug this into the posterior mean equation we get,\n",
    "$$\n",
    "\\mbox{E}\\left[\\mathbf{f}^*|y\\right] = \\Sigma_{*f}\\left(\\Sigma_{fu} \\Sigma_{uu}^{-1} \\Sigma_{uf} + D\\right)^{-1} y\n",
    "$$\n",
    "Which we've already determined is not actually computationally helpful. Using the Nystrom approximation again and using $Q_{*f}$ instead of $\\Sigma_{*f}$ gives us,\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mbox{E}\\left[\\mathbf{f}^*|y\\right] &\\approx Q_{*f}\\left(\\Sigma_{fu} \\Sigma_{uu}^{-1} \\Sigma_{uf} + D\\right)^{-1} y \\\\\n",
    "&= \\Sigma_{*u}\\Sigma_{uu}^{-1}\\Sigma_{uf}\\left(\\Sigma_{fu} \\Sigma_{uu}^{-1} \\Sigma_{uf} + D\\right)^{-1} y \\\\\n",
    "&= \\Sigma_{*u}\\left(\\Sigma_{uu} + \\Sigma_{uf} D^{-1} \\Sigma_{fu}\\right)^{-1} \\Sigma_{uf} D^{-1} y\n",
    "\\end{align}\n",
    "$$\n",
    "That last step is probably not obvious, but it's a variation on the matrix inversion lemma (See [The Matrix Cookbook](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf) for details). Below we'll write a function which produces mean predictions using this sparse approximation. The predictive covariance can be derived using a similar approach,\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mbox{COV}\\left[\\mathbf{f}^*|y\\right] &= K_{**} - K_{*u} K_{uu}^{-1} K_{u*} + K_{*u} \\left(\\Sigma_{uu} + \\Sigma_{uf} D^{-1} \\Sigma_{fu}\\right)^{-1} K_{u*}\n",
    "\\end{align}\n",
    "$$\n",
    "Now let's actually implement a sparse solver. To summarize we'll need to:\n",
    "- Construct the covariance matrices $K_{uu}$, $K_{uf}$, $K_{*u}$, and $K_{ss}$ using the covariance function.\n",
    "- Compute $D = \\mbox{diag}[K_{ff} - Q_{ff} + \\sigma_{\\epsilon}^2]$ which will require building $Q_{ff} = K_{fu} K_{uu}^{-1} K_{uf}$ (or better yet, just finding the diagonal of the result)\n",
    "- Compute $S = K_{uu} + K_{uf} D^{-1} K_{fu}$\n",
    "- Compute $\\mu = K_{*u} S^{-1} K_{uf} D^{-1} y$\n",
    "- Compute $V = K_{**} - K_{**} - K_{*u} K_{uu}^{-1} K_{u*} + K_{*u} S^{-1} K_{u*}$\n",
    "There are a lot of different ways to go about this, and plenty of room for optimization ... but probably best to ignore such optimizations for this example.\n",
    "- Return $\\mathbf{f^*}|y \\sim \\mathcal{N}(\\mu, V)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc56cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper to solve D^-1 b when D is a vector representing diagonal elements\n",
    "def diagonal_solve(D, b):\n",
    "    b = np.array(b)\n",
    "    if b.ndim == 1:\n",
    "        return b / D\n",
    "    else:\n",
    "        return b / D[:, None]\n",
    "\n",
    "# A helper to evaluate a covariance function to obtain only the\n",
    "# diagonal elements of a covariance matrix.\n",
    "#\n",
    "# diagonal_covariance(cov_func, X) == np.diag(cov_func(X, X))\n",
    "def diagonal_variance(cov_func, X):\n",
    "    return np.array([cov_func(X[i], X[i])[0, 0]\n",
    "                     for i in range(X.shape[0])])\n",
    "\n",
    "\n",
    "def sparse_fit_and_predict(cov_func, X, y, u, x_star, meas_noise):\n",
    "    # K_uu = \n",
    "    # K_uf = \n",
    "    # K_ff_diag = \n",
    "    # K_su = \n",
    "    # K_ss = \n",
    "\n",
    "    # Find the diagonal of Q_ff = K_fu K_uu^-1 K_uf\n",
    "    # Q_ff_diag = \n",
    "    # D = \n",
    "    # S = \n",
    "    \n",
    "    # mean = \n",
    "    # cov = \n",
    "    return mean, cov\n",
    "\n",
    "u = np.array([0.])\n",
    "\n",
    "# example call to fit and predict:\n",
    "# sparse_fit_and_predict(constant_covariance, X, y, u, x_star, MEAS_NOISE)\n",
    "\n",
    "TEST_SPARSE_FIT_AND_PREDICT(sparse_fit_and_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460efa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit sparse_fit_and_predict(constant_covariance, X, y, u, x_star, MEAS_NOISE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bc91fe",
   "metadata": {},
   "source": [
    "Out generalized solver may not be quite as fast as using the extremely optimized version we did above (which took advantage of known structure to compute inversions analyticaly) but it should still be an order of magnitude faster than the dense solver.\n",
    "\n",
    "The next question is how to choose the inducing points, $u$. In the constant examples, we just set $u$ to be the constant offset. But it's much more common to pick points spanning the domain. How many points are used and where exactly they get placed is very problem dependent. Here's an interactive plot, be sure to change the number of inducing points to see how that changes the prediction,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b44cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_cov_func = partial(example_squared_exponential, ell=3., sigma=2.)\n",
    "\n",
    "def plot_fit_and_predict(n_measurements, n_inducing_points):\n",
    "    xs = X[:int(n_measurements)]\n",
    "    ys = y[:int(n_measurements)]\n",
    "    U = np.linspace(LOW, HIGH, n_inducing_points)\n",
    "    fit_model = example_sparse_fit(example_cov_func, xs, ys, U, meas_noise=0.35)\n",
    "    pred_mean, pred_cov = example_sparse_predict(fit_model, x_gridded)\n",
    "    plot_spread(x_gridded, pred_mean, np.diag(pred_cov))\n",
    "    plot_truth(x_gridded)\n",
    "    plot_measurements(xs, ys)\n",
    "\n",
    "    inducing_mean, _ = example_sparse_predict(fit_model, U)\n",
    "    plt.scatter(U, inducing_mean, s=200,\n",
    "                edgecolor=\"white\", color=\"forestgreen\",\n",
    "                zorder=100, label=\"inducing points\")\n",
    "    plt.ylim([-1, 5])\n",
    "    plt.legend()\n",
    "\n",
    "@interact\n",
    "def interactive_plot_fit_and_predict(n_measurements=(10, 1000., 10),\n",
    "                                     n_inducing_points=(1, 15, 1)):\n",
    "    plot_fit_and_predict(n_measurements, n_inducing_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d137f8c",
   "metadata": {},
   "source": [
    "# How Many Inducing Points Are Needed?\n",
    "\n",
    "One way to get a sense for how many inducing points are needed for a given problem is to actually build the approximate prior and compare it to the actual prior. Here are some images showing the actual prior for gridded X, along side the approximate prior and then the difference between the two when we use five inducing points,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68a01d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nystrom_approximation(cov_func, n):\n",
    "    u = np.linspace(np.min(x_gridded), np.max(x_gridded), n)\n",
    "    K_uu = cov_func(u, u)\n",
    "    K_uf = cov_func(u, x_gridded)\n",
    "    Q_ff = K_uf.T @ np.linalg.solve(K_uu, K_uf)\n",
    "    return Q_ff\n",
    "\n",
    "\n",
    "def plot_nystrom_approximation(n):\n",
    "    K_ff = example_cov_func(x_gridded, x_gridded)\n",
    "    Q_ff = nystrom_approximation(example_cov_func, n)\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(24, 6))\n",
    "    axes[0].imshow(K_ff)\n",
    "    axes[0].set_title(\"Actual Prior\")\n",
    "\n",
    "    axes[1].imshow(Q_ff)\n",
    "    axes[1].set_title(\"Approximate Prior\")\n",
    "\n",
    "    axes[2].imshow(K_ff - Q_ff, cmap=\"coolwarm\",\n",
    "                   norm=plt.Normalize(vmin=-0.5, vmax=0.5))\n",
    "    _ = axes[2].set_title(\"Error\")\n",
    "\n",
    "plot_nystrom_approximation(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94f943b",
   "metadata": {},
   "source": [
    "It's very obvious the approximate prior varies significantly from the actual prior. But as we increase the number of inducing points that changes,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181de1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_nystrom_approximation(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33f3735",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_nystrom_approximation(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55b78f1",
   "metadata": {},
   "source": [
    "It appears that with 7 inducing points our approximate priori is very nearly matching the actual prior. This sort of analysis can be a very helpful tool for deciding if you've designed a model which _should_ provide a good approximation to the full problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39f15a5",
   "metadata": {},
   "source": [
    "Just to triple check we can compare the sparse GP prediction to the dense GP,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96082f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = np.linspace(LOW, HIGH, 7)\n",
    "sparse_mean, sparse_cov = example_sparse_fit_and_predict(example_cov_func, X, y, u, x_gridded, meas_noise=0.35)\n",
    "dense_mean, dense_cov = example_fit_and_predict(example_cov_func, X, y, x_gridded, meas_noise=0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7714cd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(15, 15))\n",
    "plot_measurements(X, y)\n",
    "truth = plot_truth(x_gridded)\n",
    "sparse = plot_spread(x_gridded, sparse_mean, np.diag(sparse_cov), ax=ax, color=\"forestgreen\")\n",
    "dense = plot_spread(x_gridded, dense_mean, np.diag(dense_cov), ax=ax, color=\"steelblue\")\n",
    "ax.legend(handles=[truth, sparse, dense], labels=[\"truth\", \"sparse\", \"dense\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c289155a",
   "metadata": {},
   "source": [
    "The sparse fit is indistiguishable from the truth inside the range spanned by the measurements. Outside that range we didn't place any inducing points, so the predictions differ a bit, but overall this is probably a satsifactory approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccd2f47",
   "metadata": {},
   "source": [
    "# How Much Faster \n",
    "\n",
    "Next question, does using this approximation help speed up the training process?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799b4e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure the predict step.\n",
    "def do_sparse_fit_predict(cov_func, X, y, x_test, meas_noise):\n",
    "    u = np.linspace(LOW, HIGH, 7)\n",
    "    return example_sparse_fit_and_predict(cov_func, X, y, u, x_test, meas_noise)\n",
    "\n",
    "sizes, sparse_times = zip(*generate_timings(do_sparse_fit_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c79158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure the predict step.\n",
    "def do_dense_fit_predict(cov_func, X, y, x_test, meas_noise):\n",
    "    return example_fit_and_predict(cov_func, X, y, x_test, meas_noise)\n",
    "\n",
    "sizes, dense_times = zip(*generate_timings(do_dense_fit_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b93278",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sizes, dense_times, label=\"dense\")\n",
    "plt.plot(sizes, sparse_times, label=\"sparse\")\n",
    "plt.xlabel(\"Number of Measurements\")\n",
    "plt.ylabel(\"Seconds\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fe9d2f",
   "metadata": {},
   "source": [
    "For this particular example a small number of inducing points seems to nearly perfectly capture the prior and the result is a MASSIVE speedup. In practice the number of inducing points required (and what exactly they represent) is problem specific, but if you find yourself in a situation where training a GP and the model fitting is taking too long, a sparse approximation like FITC/PITC/VFE could be a good option.\n",
    "\n",
    "A few notes:\n",
    "- In this example we simply prescribed evenly spaced inducing points. A lot of the litterature includes the inducing points as hyper parameters and will solve for the parameters (like length scale etc) AND the locations of the inducing points which collectively maximize the likelihood (or other objective)\n",
    "- One nice thing about these sparse approximation approaches is that you don't neccesarily need to change the model. If you're fitting a dense GP using a squared exponential covariance, you can switch to a sparse GP and retain the same model. Provided you've properly selected inducing points the result _should_ be identical but just better prepared for scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5f7485",
   "metadata": {},
   "source": [
    "Remember at the beginning we guessed it would take an hour to fit with 100k data points? Let's try it with the sparse GP,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fb907c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_large, y_large = generate_training_data(100000)\n",
    "plot_measurements(X_large, y_large)\n",
    "plot_truth(x_gridded)\n",
    "sparse_mean, sparse_cov = example_sparse_fit_and_predict(example_cov_func, X, y, u, x_gridded, meas_noise=0.35)\n",
    "plot_spread(x_gridded, sparse_mean, np.diag(sparse_cov))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f7a4b6",
   "metadata": {},
   "source": [
    "It probably took longer to generate the plot than it did to fit the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4477eeee",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
