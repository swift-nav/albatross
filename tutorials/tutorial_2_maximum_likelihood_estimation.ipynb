{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b293d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import scipy\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import gpflow\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from tutorial_utils import *\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c1f44f",
   "metadata": {},
   "source": [
    "# Maximum Likelihood (1D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca36e508",
   "metadata": {},
   "source": [
    "To start let's consider an very simple toy problem. Let's say we have some measurements from a normal distribution with unknown mean and variance,\n",
    "$$\n",
    "\\mathbf{y} \\sim \\mathcal{N}\\left(\\mu, \\sigma^2\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a965a4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2012)\n",
    "mu = np.pi\n",
    "sigma = np.log(2)\n",
    "actual_dist = scipy.stats.norm(loc=mu, scale=sigma)\n",
    "y_train = actual_dist.rvs(size=10)\n",
    "_ = plt.hist(y_train, bins=21)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25321889",
   "metadata": {},
   "source": [
    "now we'd like to estimate $\\hat{\\mu}$ and $\\hat{\\sigma}$ which \"best\" describe the data. One way would be to propose a value for $\\hat{\\mu}$ and $\\hat{\\sigma}$,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de861e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "proposed_dist = scipy.stats.norm(loc=2., scale=1.)\n",
    "xs = np.linspace(-2., 6., 101)\n",
    "plt.fill(xs, proposed_dist.pdf(xs), label=\"proposed\", alpha=0.5)\n",
    "plt.scatter(y_train, np.zeros(y_train.size), color='black', label=\"measurements\")\n",
    "for y_i in y_train:\n",
    "    plt.plot([y_i, y_i], [0, proposed_dist.pdf(y_i)], color='firebrick')\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95475802",
   "metadata": {},
   "source": [
    "Here the red lines show the probability density (PDF) according to the proposed distribution (in blue) for the data points. You can then ask for the likelihood one of the data points came from the proposed distribution. Likelihood basically just means the probability density. We can evaluate the PDF for a single observation, $y_i$,\n",
    "$$\n",
    "p(y_i; \\hat{\\mu}, \\hat{\\sigma}) = \\frac{1}{\\hat{\\sigma}\\sqrt{2\\pi}} \\mbox{exp}\\left[-\\frac{(y_i - \\hat{\\mu})^2}{2 \\hat{\\sigma}^2}\\right]\n",
    "$$\n",
    "Since $y_i$ and $y_j$ are independent, the probability that both $y_i$ _and_ $y_j$ came from the distribution you multiply,\n",
    "$$\n",
    "p(y_i, y_j) = p(y_i) p(y_j)\n",
    "$$\n",
    "Repeating for all the data gives us,\n",
    "$$\n",
    "p(y_0, \\ldots, y_n; \\hat{\\mu}, \\hat{\\sigma}) = \\prod_i^n \\frac{1}{\\hat{\\sigma}\\sqrt{2\\pi}} \\mbox{exp}\\left[-\\frac{(y_i - \\hat{\\mu})^2}{2 \\hat{\\sigma}^2}\\right]\n",
    "$$\n",
    "This involves evaluating the probability density at each data point and then taking the product of them all. Let's compute that below. Note that the `scipy` distributions have a `dist.pdf()` method, so no need to implement that your self."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4288086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_independent_likelihood(dist, data):\n",
    "    # YOUR CODE HERE\n",
    "    # pdfs = \n",
    "    # likelihood = \n",
    "    return likelihood # (scalar)\n",
    "\n",
    "TEST_COMPUTE_INDEPENDENT_LIKELIHOOD(compute_independent_likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5504bd7b",
   "metadata": {},
   "source": [
    "and then take the product to get the overall probability the data,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee5f150",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_independent_likelihood(proposed_dist, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ac7318",
   "metadata": {},
   "source": [
    "Notice that the likelihood is _VERY_ small. Check out what happens when we have even more data,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb697cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dist.rvs(n) method will sample n random variables from the distribution\n",
    "compute_independent_likelihood(actual_dist, actual_dist.rvs(size=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84aa1538",
   "metadata": {},
   "source": [
    "Notice that we drew samples from a distribution, then asked for the likelihood those samples came from the actual distribution ... yet we still got a zero! The problem is that the likelihood of each data point is smaller than 1, so the product of lots of small numbers converges to zero. Now check this out,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cd4a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "tight_dist = scipy.stats.norm(loc=0., scale=1e-6)\n",
    "compute_independent_likelihood(tight_dist, tight_dist.rvs(size=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35d45b6",
   "metadata": {},
   "source": [
    "This time we got infinite likelihood! In this case, because the scale ($\\sigma$) of the distribution was so small, the density actually takes on really large values, here's the peak probability density for the tight distribution,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4a3dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tight_dist.pdf(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ebbb78",
   "metadata": {},
   "source": [
    "While a distribution with a larger scale return density values which are smaller,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551feced",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_dist = scipy.stats.norm(loc=0., scale=1000)\n",
    "wide_dist.pdf(wide_dist.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e10479",
   "metadata": {},
   "source": [
    "The point here is that directly working with likelihood means multiplying lots of values together and those values could be large or small which can quickly lead to numerically zero or numerically infinite likelihood computations. to alleviate this problem we can look at log likelihoods,\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mbox{log}[p(y)] &= \\mbox{log}\\left[\\prod_i^n \\frac{1}{\\hat{\\sigma}\\sqrt{2\\pi}} \\mbox{exp}\\left(-\\frac{(y_i - \\hat{\\mu})^2}{2 \\hat{\\sigma}^2}\\right)\\right] \\\\\n",
    "&= \\sum_i^n \\mbox{log}\\left[\\frac{1}{\\hat{\\sigma}\\sqrt{2\\pi}} \\mbox{exp}\\left(-\\frac{(y_i - \\hat{\\mu})^2}{2 \\hat{\\sigma}^2}\\right)\\right] \\\\\n",
    "&= - n \\mbox{log}[\\hat{\\sigma}] - \\frac{n}{2} \\mbox{log}[2 \\pi] - \\sum_i^n \\frac{(y_i - \\hat{\\mu})^2}{2 \\hat{\\sigma}^2}\n",
    "\\end{align}\n",
    "$$\n",
    "Let's try implementing that, feel free to do it from scratch, or `scipy` also has a helper for the log PDF, `dist.logpdf()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2153ce1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_independent_log_likelihood(dist, data):\n",
    "    # YOUR CODE HERE\n",
    "    # log_pdfs = \n",
    "    # log_likelihood = \n",
    "    return log_likelihood # (scalar)\n",
    "\n",
    "TEST_COMPUTE_INDEPENDENT_LOG_LIKELIHOOD(compute_independent_log_likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9a2de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_independent_log_likelihood(tight_dist, tight_dist.rvs(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbcfe6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_independent_log_likelihood(wide_dist, wide_dist.rvs(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd583fd",
   "metadata": {},
   "source": [
    "The two likelihood computations are very different magnitudes, but by taking the log of the likelihood we have more numerically stable results.\n",
    "\n",
    "Now that we can compute the likelihood that the data came from a model with given parameters, we can try to find the parameters which maximize that likelihood. `scipy` also has an `optimize` package which provides some methods we can use to find the argument which minimize a function. The interface requires an objective function that takes a numpy array as a single argument and returns a scalar.\n",
    "```\n",
    "def objective_to_minimize(x):\n",
    "    return scalar\n",
    "```\n",
    "After which you can use,\n",
    "```\n",
    "arguments_which_minimize = scipy.optimize.minimize(objective_to_minimize).x\n",
    "```\n",
    "which, as the name suggests, will _minimize_ the function. If you want to maximize the function you'll need your objective function to return the negative of your actual objective,\n",
    "```\n",
    "def objective_to_minimize(x):\n",
    "    return -objective_to_maximize(x)\n",
    "```\n",
    "This is why it's common to talk about the negative log likelihood (NLL), not the log likelihood. Let's write a function which takes a mean and standard deviation as input and returns the negative log likelihood that the data came from the proposed distribution,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad63447a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_independent_negative_log_likelihood(params, ys):\n",
    "    mu, sigma = params\n",
    "    # YOUR CODE HERE\n",
    "    #\n",
    "    # first, build a scipy distribution from the parameters\n",
    "    # distribution =\n",
    "    #\n",
    "    # then use it to compute the likelihood\n",
    "    # log_likelihood = \n",
    "    #\n",
    "    return -log_likelihood\n",
    "\n",
    "TEST_COMPUTE_INDEPENDENT_NEGATIVE_LOG_LIKELIHOOD(compute_independent_negative_log_likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99d6efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mle_mu, mle_sigma = scipy.optimize.minimize(compute_independent_negative_log_likelihood,\n",
    "                                            x0=[1., 1.],\n",
    "                                            args=(y_train,),\n",
    "                                            bounds=[(1e-6, 100), (1e-6, 100)]).x\n",
    "print(f\"mean -- actual: {actual_dist.mean():.3f}, MLE estimate: {mle_mu:.3f}\")\n",
    "print(f\"std  -- actual: {actual_dist.std():.3f}, MLE estimate: {mle_sigma:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b95dbf",
   "metadata": {},
   "source": [
    "Not bad, the maximum likelihood estimate of the mean and standard deviation are pretty close to the true values, particularly given the small sample size. Here's what we'd get with a much larger dataset,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fca25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_mle_mu, large_mle_sigma = scipy.optimize.minimize(compute_independent_negative_log_likelihood,\n",
    "                                            x0=[1., 1.],\n",
    "                                            args=(actual_dist.rvs(size=10000),),\n",
    "                                            bounds=[(1e-6, 100), (1e-6, 100)]).x\n",
    "print(f\"mean -- actual: {actual_dist.mean():.3f}, MLE estimate: {large_mle_mu:.3f}\")\n",
    "print(f\"std  -- actual: {actual_dist.std():.3f}, MLE estimate: {large_mle_sigma:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e17198",
   "metadata": {},
   "source": [
    "Fun fact, instead of building a log likelihood function and doing a computationally intense round of numerical optimization to learn the maximum likelihood estimates of the mean and standard devation we could have just computed the mean and standard deviation of the sample data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22778c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(y_train), mle_mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b329e9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.std(y_train), mle_sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9707b04",
   "metadata": {},
   "source": [
    "For normal distributions moment matching (setting the mean and variance to the sample mean and variance) is equivalent to maximum likelihood estimation! So in this toy problem, solving from the params that maximize likelihood is overkill, but this concept using optimization routines to find the parameters that maximize the likelihood is more generally useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fbf355",
   "metadata": {},
   "source": [
    "# Maximum Likelihood (Gaussian Processes)\n",
    "\n",
    "You may remember from the first tutorial, we defined a GP with a squared exponential covariance, and then solved for the parameters ($\\sigma_{se}$, $\\ell$ and $\\sigma_{\\epsilon}$) which maximized the likelihood of the data. That implementation was _very_ similar to the 1D case we just implemented, but let's generalize it here. In the 1D example, we picked values, $\\hat{\\mu}$ and $\\hat{\\sigma}$, and proposed a distribution,\n",
    "$$\n",
    "y_i = \\mathcal{N}\\left(\\hat{\\mu}, \\hat{\\sigma}^2\\right)\n",
    "$$\n",
    "then maximized the likelihood that $y = \\{y_0, \\ldots, y_n\\}$ came from that distribution.\n",
    "\n",
    "When we build a Gaussian process we start with a general form for a covariance function, $c(x, x')$. That covariance function might have its own hyper parameters, $\\theta$. This is often written $c(x, x'; \\theta)$ to indicate that $x$ and $x'$ are the arguments, while $\\theta$ are hyper parameters which change the behavior of $c$. Take the squared exponential as an example, that function can be written,\n",
    "$$\n",
    "c(x_i, x_j; \\sigma, \\ell) = \\sigma^2 \\mbox{exp}\\left(-\\frac{\\left|x_i - x_j\\right|^2}{\\ell^2}\\right).\n",
    "$$\n",
    "Let's setup an optimization problem which let's us solve for the maximum likelihood hyper parameters. Much like the 1D problem we'll need to write a function which takes arbitrary values for $\\sigma$ and $\\ell$ and returns the likelihood that our data came from the proposed distribution. First, the proposed distribution for a given set of parameters,\n",
    "$$\n",
    "\\mathbf{y} \\sim \\mathcal{N}\\left(0, \\Sigma_{yy}(\\sigma, \\ell) + \\sigma_{\\epsilon}^2 I\\right)\n",
    "$$\n",
    "Notice that we've assumed the mean is zero again, $\\Sigma_{yy}$ depends on the hyper parameters $\\sigma$ and $\\ell$, and there's actually a third hyper parameter $\\sigma_{\\epsilon}$ which describes the measurement noise. For simplicity, let's drop the subscripts and call $\\Sigma$ the resulting covariance matrix for a given set of hyper parameters,\n",
    "$$\n",
    "\\Sigma = \\Sigma_{yy}(\\sigma, \\ell) + \\sigma_{\\epsilon}^2 I\n",
    "$$\n",
    "giving us,\n",
    "$$\n",
    "\\mathbf{y} \\sim \\mathcal{N}\\left(0, \\Sigma\\right)\n",
    "$$\n",
    "we now need to compute the likelihood that actual measurements $y$ came from the proposed distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dff157",
   "metadata": {},
   "source": [
    "At the end of the notebook I added a section where I describe a way to intuitively get from the definition of likelihood for independent normal random variables to correlated multivariate normal variables, you could take a look at that ... or just [look on wikipedia](https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Likelihood_function).\n",
    "\n",
    "The multivariate log likelihood function is,\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mbox{log}(p(y)) &= \\mbox{log}\\left(|\\Sigma|^{-1/2} (2\\pi)^{-n/2} \\mbox{exp}\\left[-\\frac{y^T \\Sigma^{-1} y}{2}\\right]\\right) \\\\\n",
    "& = -\\frac{1}{2} \\mbox{log}\\left(|\\Sigma|\\right) - \\frac{n}{2} \\mbox{log}(2\\pi) - \\frac{y^T \\Sigma^{-1} y}{2}\n",
    "\\end{align}\n",
    "$$\n",
    "There are a lot of different ways you could do this, but some tips you might find useful are:\n",
    "- `np.linalg.slogdet(A)`. which returns a pair `sign, log_det`. In this case the `sign` should always be positive, so you probably only care about the second return value: `log_det = np.linalg.slogdet(A)[1]`\n",
    "- `scipy.stats.multivariate_normal.logpdf`. this one is definitely cheating ... but if you're short on time go for it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e69f785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_mvn_log_likelihood(S, y)\n",
    "#\n",
    "#   Compute the log likelihood for a sample y, from a\n",
    "#   multivariate normal (mvn) distribution with covariance S\n",
    "#   and mean assumed zero.\n",
    "def compute_mvn_log_likelihood(S, y):\n",
    "    # YOUR CODE HERE\n",
    "    #\n",
    "    # log_det_term =\n",
    "    # constant_term = \n",
    "    # squared_error_term = \n",
    "    # log_likelihood = \n",
    "    return log_likelihood\n",
    "\n",
    "TEST_COMPUTE_MVN_LOG_LIKELIHOOD(compute_mvn_log_likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e04963",
   "metadata": {},
   "source": [
    "Let's recreate the one dimensional Gaussian process example from the first tutorial,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3580cc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2012)\n",
    "N = 101\n",
    "X_train = np.random.uniform(LOW, HIGH, size=N)\n",
    "y_train = truth(X_train) + MEAS_NOISE * np.random.normal(size=N)\n",
    "X_gridded = np.linspace(LOWEST, HIGHEST, 101)\n",
    "\n",
    "plot_truth(X_gridded)\n",
    "plot_measurements(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d5ea17",
   "metadata": {},
   "source": [
    "We can now propose different measurement noise, $\\sigma_{\\epsilon}^2$, length scales, $\\ell$, and variances, $\\sigma^2$ to get a sense of how sensitive the likelihood is to the hyper parameters. First, let's fix the variance and measurement noise,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c6d275",
   "metadata": {},
   "outputs": [],
   "source": [
    "ells = np.linspace(0.5, 5, 51)\n",
    "\n",
    "def eval(ell, sigma, meas_noise=MEAS_NOISE):\n",
    "    cov = example_squared_exponential(X_train, X_train, ell=ell, sigma=sigma)\n",
    "    cov += np.square(meas_noise) * np.eye(y_train.size)\n",
    "    return compute_mvn_log_likelihood(cov, y_train)\n",
    "\n",
    "log_likelihoods = np.vectorize(eval)(ells, 1.)\n",
    "\n",
    "plt.plot(ells, log_likelihoods)\n",
    "plt.xlabel(\"Length Scale\")\n",
    "plt.ylabel(\"Log Likelihood\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e63472",
   "metadata": {},
   "source": [
    "With the variances held fixed, we see there's a strong dependence on the length scale. But the likelhood also depends strong on the $\\sigma$ we pick,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f12d029",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmas = np.linspace(0.5, 5, 51)\n",
    "log_likelihoods = np.vectorize(eval)(2., sigmas)\n",
    "\n",
    "plt.plot(sigmas, log_likelihoods)\n",
    "plt.xlabel(\"sigma\")\n",
    "plt.ylabel(\"Log Likelihood\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9aeb127",
   "metadata": {},
   "source": [
    "We can plot both the likelihood with respect to both of these variables as a surface,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f963f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_ells, grid_sigmas = np.meshgrid(ells, sigmas)\n",
    "\n",
    "log_likelihoods = np.vectorize(eval)(grid_ells, grid_sigmas)\n",
    "\n",
    "# Create a 3D plot\n",
    "fig = plt.figure(figsize=(24, 8))\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "ax.plot_surface(grid_ells, grid_sigmas, log_likelihoods)\n",
    "ax.set_xlabel(\"Length Scale\")\n",
    "ax.set_ylabel(\"Sigma\")\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "plt.pcolor(grid_ells, grid_sigmas, log_likelihoods)\n",
    "ax.set_xlabel(\"Length Scale\")\n",
    "ax.set_ylabel(\"Sigma\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a946c4",
   "metadata": {},
   "source": [
    "We could just look at these plots and get a pretty good idea of which values to use, but then we'd be ignoring the third variable. Instead we can use the same numerical optimization technique from the univariate case. To do so we need our objective function. We'll assume we're using a simple GP with a squared exponential covariance, no need to re-implement the covariance function, feel free to use `example_squared_exponential(x_i, x_j, sigma, ell)`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e63c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sqr_exp_negative_log_likelihood(params, X, y):\n",
    "    sigma, ell, meas_noise = params\n",
    "    # Compute S = S_yy + meas_noise^2 I\n",
    "    # S =\n",
    "    # log_likelihood =    \n",
    "    return -log_likelihood\n",
    "\n",
    "TEST_COMPUTE_SQR_EXP_NLL(compute_sqr_exp_negative_log_likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc20ec3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.ones(3) # initialize parameters with something reasonable\n",
    "bounds = [(1e-6, 100)] * 3 # we need to keep the hyper-parameters positive.\n",
    "mle_params = scipy.optimize.minimize(compute_sqr_exp_negative_log_likelihood,\n",
    "                                     x0=x0, method=\"L-BFGS-B\",\n",
    "                                     args=(X_train, y_train),\n",
    "                                     bounds=bounds)\n",
    "mle_params = dict(zip([\"sigma\", \"ell\", \"noise\"], mle_params.x))\n",
    "print(mle_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8d44fa",
   "metadata": {},
   "source": [
    "Just to make sure, we can show our maximum likelihood estimates on top of the likelihood surface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e75a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = fig.add_subplot(122)\n",
    "\n",
    "log_likelihoods = np.vectorize(eval)(grid_ells, grid_sigmas, mle_params[\"noise\"])\n",
    "plt.pcolor(grid_ells, grid_sigmas, log_likelihoods)\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"Length Scale\")\n",
    "plt.ylabel(\"Sigma\")\n",
    "plt.scatter(mle_params[\"ell\"], mle_params[\"sigma\"],\n",
    "            s=500, color=\"steelblue\", marker=\"*\")\n",
    "plt.text(mle_params[\"ell\"], mle_params[\"sigma\"] + 0.2, \"MLE\",\n",
    "         fontsize=\"x-large\", ha=\"center\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73e335c",
   "metadata": {},
   "source": [
    "At this point we've implemented a lot of the math you'd find in a typical Gaussian process package. Don't believe me? We can compare our MLE estimated parameters to those estimated by `gpflow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19b0486",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_gpflow = X_train[:, None].copy()\n",
    "y_gpflow = y_train[:, None].copy()\n",
    "model = gpflow.models.GPR(\n",
    "    (X_train[:, None], y_train[:, None]),\n",
    "    kernel=gpflow.kernels.SquaredExponential(),\n",
    ")\n",
    "opt = gpflow.optimizers.Scipy()\n",
    "opt.minimize(model.training_loss, model.trainable_variables)\n",
    "\n",
    "gpflow_params = {\n",
    "    \"sigma\": np.sqrt(model.kernel.variance.numpy()),\n",
    "    \"ell\": model.kernel.lengthscales.numpy(),\n",
    "    \"noise\": np.sqrt(model.likelihood.variance.numpy())\n",
    "}\n",
    "\n",
    "pd.DataFrame({\"ours\": mle_params, \"gpflow\": gpflow_params})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0c8668",
   "metadata": {},
   "source": [
    "We had to convert from variance to standard deviation, but the hyper parameters are equivalent!\n",
    "\n",
    "\n",
    "\n",
    "A lot of the remaining complexity in packages (including `albatross`) consists of generalizations to help a user compose covariance functions, keep track of all the hyper parameters, and provide the building blocks which allow users to write custom objective functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67d14f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mle_cov_func = partial(example_squared_exponential,\n",
    "                       sigma=mle_params[\"sigma\"], ell=mle_params[\"ell\"])\n",
    "\n",
    "mle_mean, mle_cov = example_fit_and_predict(mle_cov_func, X_train, y_train,\n",
    "                                            X_gridded, meas_noise=mle_params[\"noise\"])\n",
    "\n",
    "gpflow_mean, gpflow_var = model.predict_f(X_gridded[:, None])\n",
    "fig, axes = plt.subplots(1, 2, figsize=(24, 8))\n",
    "axes[0].set_title(\"Our Impementation\")\n",
    "plot_spread(X_gridded, mle_mean, np.sqrt(np.diag(mle_cov)), ax=axes[0])\n",
    "axes[1].set_title(\"gpflow\")\n",
    "plot_spread(X_gridded, gpflow_mean, np.sqrt(gpflow_var), ax=axes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d406a94",
   "metadata": {},
   "source": [
    "# Appendix:\n",
    "\n",
    "### Intuition Behind Multivariate Normal Likelihoods\n",
    "\n",
    "Understanding this section isn't entirely neccesary, and the math is a little hand wavey, but it might help build some intuition around the likelihood of a multivariate normal distribution,\n",
    "$$\n",
    "\\mathbf{y} \\sim \\mathcal{N}\\left(0, \\Sigma\\right)\n",
    "$$\n",
    "it might be tempting to do the same thing we did in one dimension and first compute $p(y_i)$ using the diagonal element of $\\Sigma_{ii}$ for each measurement, then form $p(y) = \\prod_i p(y_i)$, but that doesn't work anymore! Simply multiplying marginal likelihoods like that assumes each $y_i$ is independent of all the others, but because we assume $\\Sigma$ is dense we now have correlated measurements. In the first tutorial we started with independent random variables,\n",
    "$$\n",
    "\\mathbf{z} \\sim \\mathcal{N}\\left(0, I\\right)\n",
    "$$\n",
    "and correlated them,\n",
    "$$\n",
    "\\begin{align}\n",
    "L \\mathbf{z} &\\sim \\mathcal{N}\\left(0, L L^T\\right) \\\\\n",
    "& \\sim \\mathcal{N}\\left(0, \\Sigma \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "using the cholesky ($L$ such that $L L^T = \\Sigma$). We can actually reverse that process to de-correlate our measurements,\n",
    "$$\n",
    "\\begin{align}\n",
    "L^{-1} \\mathbf{y} &\\sim L^{-1} \\mathcal{N}\\left(0, \\Sigma\\right) \\\\\n",
    "&\\sim \\mathcal{N}\\left(0, L^{-1} \\Sigma L^{-T}\\right) \\\\\n",
    "&\\sim \\mathcal{N}\\left(0, I\\right) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "So if we're given measurements $y$ we can decorrelate them to get $z = L^{-1} y$ and can then focus on computing the likelihood for $z = \\{z_0, \\ldots, z_n\\}$ standard normal variables. For that we'll use $f(z_i)$ to mean the probability density function for, $z_i \\sim \\mathcal{N}(0, 1)$, stanard normal variables,\n",
    "$$\n",
    "\\begin{align}\n",
    "f(z_0, \\ldots, z_n) &= \\prod_i^n f(z_i) \\\\\n",
    "& = \\prod_i^n \\frac{1}{\\sqrt{2\\pi}} \\mbox{exp}\\left[-\\frac{z_i^2}{2}\\right] \\\\\n",
    "& = \\frac{1}{\\sqrt{2\\pi}^n} \\mbox{exp}\\left[-\\sum_i\\frac{z_i^2}{2}\\right] \\\\\n",
    "& = (2\\pi)^{-n/2} \\mbox{exp}\\left[-\\frac{z^T z}{2}\\right] \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "So if $L^{-1} \\mathbf{y} \\sim \\mathcal{N}(0, I)$, then maybe,\n",
    "$$\n",
    "\\begin{align}\n",
    "p(y) &= f\\left(L^{-1} y\\right) \\\\\n",
    "&= (2\\pi)^{-n/2} \\mbox{exp}\\left[-\\frac{y^T L^{-T} L^{-1} y}{2}\\right] \\\\\n",
    "&= (2\\pi)^{-n/2} \\mbox{exp}\\left[-\\frac{y^T \\Sigma^{-1} y}{2}\\right] \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "This is _almost_ right (but is NOT true!) The integral of any probability density function over the whole domain needs to equal one and when we multiplied $y$ by $L^{-1}$ we will have changed the scale. We can measure how much we will have changed the scale by computing the determinant (see an [explanation here](https://en.wikipedia.org/wiki/Determinant#Geometric_meaning)). So to make this conversion from a decorrelated probability density function to a correlated equivalent we need to also multiply by the determinant, $|L^{-1}|$:\n",
    "$$\n",
    "\\begin{align}\n",
    "p(y) &= |L^{-1}| (2\\pi)^{-n/2} \\mbox{exp}\\left[-\\frac{y^T \\Sigma^{-1} y}{2}\\right] \\\\\n",
    "p(y) &= |L|^{-1} (2\\pi)^{-n/2} \\mbox{exp}\\left[-\\frac{y^T \\Sigma^{-1} y}{2}\\right] \\\\\n",
    "&= |\\Sigma|^{-1/2} (2\\pi)^{-n/2} \\mbox{exp}\\left[-\\frac{y^T \\Sigma^{-1} y}{2}\\right]\n",
    "\\end{align}\n",
    "$$\n",
    "Where those last few steps include some identities of determinants: $|A B| = |A||B|$ and $|A^{-1}| = |A|^{-1}$.\n",
    "The takeaway: computing the likelihood for multivariate normal variables is the same as decorrelating, then computing (and rescaling) the likelihood for decorrelated normal variables."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
